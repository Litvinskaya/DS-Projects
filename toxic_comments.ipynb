{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение для текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая информация:\n",
    "Нам нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "Обучим модель классифицировать комментарии на позитивные и негативные со значением метрики качества F1 не меньше 0.75.\n",
    "\n",
    "### План работ:\n",
    "Выполнить проект без BERT .\n",
    "\n",
    "Загрузим и подготовим данные.\n",
    "Обучим разные модели.\n",
    "Сделайтем выводы.\n",
    "\n",
    "### Описание датасета:\n",
    "text- текст комментария, \n",
    "\n",
    "toxic — целевой признак.\n",
    "\n",
    "### Содержание:\n",
    "\n",
    "<a href='#1'>1.Рассмотрим и подготовим данные</a>\n",
    "\n",
    "<a href='#2'>2.Обучим разные модели.</a>\n",
    "\n",
    "<a href='#3'>3.Вывод.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\litvinskaya.m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\litvinskaya.m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoost\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "import transformers\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "m = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    txt=m.lemmatize(text)\n",
    "    text=\"\".join(txt) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    txt=re.sub(r'[^a-zA-Z]',' ', text)\n",
    "    txt=txt.split() \n",
    "    txt=\" \".join(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df=pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df=pd.read_csv(r'C:\\Users\\litvinskaya.m\\Desktop\\Материалы курса Яндекс.Практикум\\ноутбуки\\спринт 3.проект 4\\toxic_comments.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Рассмотрим и подготовим данные\n",
    " <a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63437</th>\n",
       "      <td>Would it be possible to merge these two aricle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19192</th>\n",
       "      <td>Again - it is obvious what is happening. I've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14063</th>\n",
       "      <td>Amended rationale to specify image usage.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic\n",
       "63437  Would it be possible to merge these two aricle...      0\n",
       "19192  Again - it is obvious what is happening. I've ...      0\n",
       "14063          Amended rationale to specify image usage.      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df['text']\n",
    "target=df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_test,target_train,target_test=train_test_split(features,\n",
    "                                          target,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer(stop_words=stop)\n",
    "features=count_vect.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop) \n",
    "features_train=count_tf_idf.fit_transform(features_train)\n",
    "features_test=count_tf_idf.transform(features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Обучим разные модели.\n",
    " <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyRegressor\n",
      "0.0\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('DummyRegressor')\n",
    "dummy_model= DummyRegressor(strategy=\"median\")\n",
    "dummy_model.fit(features_train,target_train)\n",
    "dummy_result=dummy_model.predict(features_test)\n",
    "dummy_result=pd.Series(dummy_result).astype('float')\n",
    "print(f1_score(dummy_result,target_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">DummyRegressor всегда нужен для проверки остальных моделей на адекватность</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='solver'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('LogisticRegression')\n",
    "LRmodel=None\n",
    "best_result=0\n",
    "\n",
    "for sol in tqdm(['newton-cg','lbfgs','liblinear','sag','saga'], desc='solver', leave=False):\n",
    "    model=LogisticRegression(random_state=42,solver=sol)\n",
    "    model.fit(features_train,target_train)\n",
    "    predict=model.predict(features_train)\n",
    "    predict=pd.Series(predict).astype('float')\n",
    "    result=f1_score(predict,target_train)\n",
    "    if result>best_result:\n",
    "        LRmodel=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7367792052584404\n"
     ]
    }
   ],
   "source": [
    "predict=LRmodel.predict(features_test)\n",
    "print(f1_score(predict,target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('LinearRegression')\n",
    "model_Regression=LinearRegression()\n",
    "model_Regression.fit(features_train,target_train)\n",
    "predict=model_Regression.predict(features_test)\n",
    "predict=pd.Series(predict).astype('float')\n",
    "#print(f1_score(target_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('DecisionTreeClassifier')\n",
    "best_model_Baum=None\n",
    "best_result=0\n",
    "best_depth=0\n",
    "for depth in range(1,100,3):\n",
    "    model_Baum=DecisionTreeClassifier(random_state=42,max_depth=depth)\n",
    "    model_Baum.fit(features_train,target_train)\n",
    "    result=cross_val_score(model_Baum, features_train, target_train, cv=5).mean()\n",
    "    if result>best_result:\n",
    "        best_model_Baum=model_Baum\n",
    "        best_result=result\n",
    "        best_depth=depth\n",
    "print(best_result)\n",
    "\n",
    "predict=model_Baum.predict(features_test)\n",
    "print(f1_score(target_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#print('CatBoostClassifier')\n",
    "#model_Cat =CatBoostClassifier(iterations=50,verbose=10)\n",
    "#features_train_f_cb=features_train.toarray()\n",
    "#target_train_f_cb=target_train.toarray()\n",
    "#features_test_f_cb=features_test.toarray()\n",
    "#target_test_f_cb=target_test.toarray()\n",
    "#model_Cat.fit(features_train,target_train)\n",
    "#result=cross_val_score(model_Cat, features_train_f_cb, target_train_f_cb, cv=3).mean()\n",
    "#predict=model_Cat.predict(features_test)\n",
    "#predict=pd.Series(predict).astype('int')\n",
    "#print(f1_score(target_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "print('LGBMRegressor')\n",
    "model_GBM=LGBMRegressor(class_weight='balanced')\n",
    "model_GBM.fit(features_train,target_train)\n",
    "prediction_GBM= model_GBM.predict(features_test)\n",
    "prediction_GBM=pd.Series(prediction_GBM).astype('int')\n",
    "result=f1_score(target_test,prediction_GBM)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('LGBMClassifier')\n",
    "model_GBMc=LGBMClassifier(random_state=42,n_estimators=500,boosting_type=\"goss\",max_depth=35)\n",
    "model_GBMc.fit(features_train,target_train)\n",
    "prediction_GBMc= model_GBMc.predict(features_test)\n",
    "prediction_GBMc=pd.Series(prediction_GBMc).astype('int')\n",
    "result=f1_score(target_test,prediction_GBMc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Вывод\n",
    " <a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Мощностей линейной модели не хватает.\n",
    "    \n",
    "\n",
    "Логистическая модель приятно удивляет скоростью бучения в 22 секунды, но к сожалению, не достягивает до поставленной нами цели в F1=0.75\n",
    "    \n",
    "\n",
    "DecisionTreeClassifier справляется, но работает долго и не достаточно качественно,что делает его даже хуже Логистической модели в нашем конкретном случае.\n",
    "    \n",
    "В связи с тем,что Дерево оказалось медленным и неэффективным было принято решение пропустить темтирование случайного леса, и перейти к другим моделям.\n",
    "    \n",
    "\n",
    "Лучше всех нам подошел LGBMClassifier c результатом f1=0.78\n",
    "    \n",
    "\n",
    "Отдельно хочется отметить что с таким объемом данных мы работает впервые, поэтому нужно запастись терпением.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
